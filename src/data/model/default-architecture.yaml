name: default

optimizer: AdamOptimizer
batch_size: 32
learning_rate: 0.001

layers:
  - &conv
    name: conv
    type: conv2d
    params:
      activation: relu
      filters: 16
      padding: SAME
      strides: [1, 1]
      kernel_size: [3, 3]
  - &pool
    name: pool
    type: max_pooling2d
    params:
      pool_size: [2, 2]
      strides: 1
  - *conv
  - &flat
    name: flat
    type: flatten
    params:
      null
  - &fcl
    name: fcl
    type: dense
    params:
      activation: relu
      units: 1024
  -
    name: logits
    type: dense
    params:
      units: 62
